I"£<p>Medical images like MRIs, CTs (3D images) are very similar to videos - both of them encode 2D spatial information over a 3rd dimension. Much like diagnosing abnormalities from 3D images, action recognition from videos would require capturing context from entire video rather than just capturing information from each frame.</p>

<div style="overflow: auto">
    <div style="float: left" id="volume">
    </div>
    <div id="video" style="float: right">
    </div>

</div>

<p align="center" class="caption">Fig 1: Left: Example Head CT scan. Right: Example video from a <a href="http://www.wisdom.weizmann.ac.il/%7Evision/SpaceTimeActions.html">action recognition dataset</a>. Z dimension in the CT volume is analogous to time dimension in the video.</p>

<p>In this post, I summarize the literature on action recognition from videos. The post is organized into three sections -</p>

<ol>
  <li><a href="#sec-1">What is action recognition and why is it tough</a></li>
  <li><a href="#sec-2">Overview of approaches</a></li>
  <li><a href="#sec-3">Summary of papers</a></li>
</ol>

<p><a name="sec-1"></a></p>

<h2 id="action-recognition-and-why-is-it-tough">Action recognition and why is it tough?</h2>

<p>Action recognition task involves the identification of different actions from video clips (a sequence of 2D frames) where the action may or may not be performed throughout the entire duration of the video. This seems like a natural extension of image classification tasks to multiple frames and then aggregating the predictions from each frame. Despite the stratospheric success of deep learning architectures in image classification (ImageNet), progress in architectures for video classification and representation learning has been slower.</p>

<p><em>What made this task tough?</em></p>

<ol>
  <li>
    <p><strong>Huge Computational Cost</strong>
A simple convolution 2D net for classifying 101 classes has just ~5M parameters whereas the same architecture when inflated to a 3D structure results in ~33M parameters.  It takes 3 to 4 days to train a 3DConvNet on UCF101 and about two months on Sports-1M, which makes extensive architecture search difficult and overfitting likely[<a href="#1">1</a>].</p>
  </li>
  <li>
    <p><strong>Capturing long context</strong>
Action recognition involves capturing spatiotemporal context across frames. Additionally, the spatial information captured has to be compensated for camera movement. Even having strong spatial object detection doesn‚Äôt suffice as the motion information also carries finer details. There‚Äôs a local as well as global context w.r.t. motion information which needs to be captured for robust predictions. For example, consider the video representations shown in Figure 2. A strong image classifier can identify human, water body in both the videos but the nature of temporal periodic action differentiates front crawl from breast stroke.</p>

    <p align="center">
     <img src="/assets/images/actionrec/fronststroke.gif" float="left" width="40%" />
     <img src="/assets/images/actionrec/breaststroke.gif" float="right" width="40%" />
     <br />
     <small>Fig 2: Left: Front crawl. Right: Breast stroke. Capturing temporal motion is critical to differentiate these two seemingly similar cases. Also notice, how camera angle suddenly changes in the middle of front crawl video.</small>
 </p>
  </li>
  <li><strong>Designing classification architectures</strong>
Designing architectures that can capture spatiotemporal information involve multiple options which are non-trivial and expensive to evaluate. For example, some possible strategies could be
    <ul>
      <li>One network for capturing spatiotemporal information vs. two separate ones for each spatial and temporal</li>
      <li>Fusing predictions across multiple clips</li>
      <li>End-to-end training vs. feature extraction and classifying separately</li>
    </ul>
  </li>
  <li>
    <p><strong>No standard benchmark</strong>
The most popular and benchmark datasets have been UCF101 and Sports1M for a long time. Searching for reasonable architecture on Sports1M can be extremely expensive. For UCF101, although the number of frames is comparable to ImageNet, the high spatial correlation among the videos makes the actual diversity in the training much lesser. Also, given the similar theme (sports) across both the datasets, generalization of benchmarked architectures to other tasks remained a problem. This has been solved lately with the introduction of Kinetics dataset[<a href="#2">2</a>].</p>

    <p align="center">
     <img src="http://www.thumos.info/assets/banner.gif" alt="Karpathy_fusion" width="100%" />
     <small>Sample illustration of UCF-101. <a href="http://www.thumos.info/">Source</a>.</small>
 </p>
  </li>
</ol>

<p><a name="sec-2"></a></p>

<p>It must be noted here that abnormality detection from 3D medical images doesn‚Äôt involve all the challenges mentioned here. The major differences between action recognition from medical images are mentioned as below</p>

<ol>
  <li>
    <p>In case of medical imaging, the temporal context may not be as important as action recognition. For example, detecting hemorrhage in a head CT scan could involve much less temporal context across slices. Intracranial hemorrhage can be detected from a single slice only. As opposed to that, detecting lung nodule from chest CT scans would involve capturing temporal context as the nodule as well as bronchi and vessels all look like circular objects in 2D scans. It‚Äôs only when 3D context is captured, that nodules can be seen as spherical objects as opposed to cylindrical objects like vessels</p>
  </li>
  <li>
    <p>In case of action recognition, most of the research ideas resort to using pre-trained 2D CNNs as a starting point for drastically better convergence. In case of medical images, such pre-trained networks would be unavailable.</p>
  </li>
</ol>

<h2 id="overview-of-approaches">Overview of approaches</h2>
<p>Before deep learning came along, most of the traditional CV algorithm variants for action recognition can be broken down into the following 3 broad steps:</p>

<hr />

<ol>
  <li>Local high-dimensional visual features that describe a region of the video are extracted either densely [<a href="#3">3</a>] or at a sparse set of interest points[<a href="#4">4</a> , <a href="#5">5</a>].</li>
  <li>The extracted features get combined into a fixed-sized video level description. One popular variant to the step is to bag of visual words (derived using hierarchical or k-means clustering) for encoding features at video-level.</li>
  <li>A classifier, like SVM or RF, is trained on bag of visual words for final prediction</li>
</ol>

<hr />

<p>Of these algorithms that use shallow hand-crafted features in Step 1, improved Dense Trajectories [<a href="#6">6</a>] (iDT)  which uses densely sampled trajectory features was the state-of-the-art. Simultaneously, 3D convolutions were used as is for action recognition without much help in 2013[<a href="#7">7</a>]. Soon after this in 2014, two breakthrough research papers were released which form the backbone for all the papers we are going to discuss in this post. The major differences between them was the design choice around combining spatiotemporal information.</p>

<p><a name="singlestream"></a></p>

<h4 id="approach-1-single-stream-network">Approach 1: Single Stream Network</h4>

<p>In this <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf">work</a> [June 2014], the authors - Karpathy et al. - explore multiple ways to fuse temporal information from consecutive frames using 2D pre-trained convolutions.</p>

<p align="center">
     <img src="/assets/images/actionrec/Karpathy_fusion.jpg" alt="Karpathy_fusion" width="80%" />
     <br />
     <small>Fig 3: Fusion Ideas <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf">Source</a>.</small>
 </p>

<p>As can be seen in Fig 3, the consecutive frames of the video are presented as input in all setups. <em>Single frame</em> uses single architecture that fuses information from all frames at the last stage. <em>Late fusion</em> uses two nets with shared params, spaced 15 frames apart, and also combines predictions at the end. <em>Early fusion</em> combines in the first layer by convolving over 10 frames. <em>Slow fusion</em> involves fusing at multiple stages, a balance between early and late fusion. For final predictions, multiple clips were sampled from entire video and prediction scores from them were averaged for final prediction.</p>

<p>Despite extensive experimentations the authors found that the results were significantly worse as compared to state-of-the-art hand-crafted feature based algorithms. There were multiple reasons attributed for this failure:</p>

<ol>
  <li>The learnt spatiotemporal features didn‚Äôt capture motion features</li>
  <li>The dataset being less diverse, learning such detailed features was tough</li>
</ol>

<p><a name="2stream"></a></p>

<h4 id="approach-2-two-stream-networks">Approach 2: Two Stream Networks</h4>

<p>In this pioneering <a href="https://arxiv.org/pdf/1406.2199.pdf">work</a> [June 2014] by Simmoyan and Zisserman, the authors build on the failures of the previous work by Karpathy et al. Given the toughness of deep architectures to learn motion features, authors explicitly modeled motion features in the form of stacked optical flow vectors. So instead of single network for spatial context, this architecture has two separate networks - one for spatial context (pre-trained), one for motion context. The input to the spatial net is a single frame of the video. Authors experimented with the input to the temporal net and found bi-directional optical flow stacked across for 10 successive frames was performing best. The two streams were trained separately and combined using SVM. Final prediction was same as previous paper, i.e. averaging across sampled frames.</p>

<p align="center">
    <img src="/assets/images/actionrec/2stream_high.png" alt="2 stream architecture" height="200" width="100%" />
    <br />
    <small>Fig 4: Two stream architecture <a href="https://arxiv.org/pdf/1406.2199.pdf">Source</a>.</small>
</p>

<p>Though this method improved the performance of single stream method by explicitly capturing local temporal movement, there were still a few drawbacks:</p>

<ol>
  <li>Because the video level predictions were obtained from averaging predictions over sampled clips, the long range temporal information was still missing in learnt features.</li>
  <li>Since training clips are sampled uniformly from videos, they suffer from a problem of <em>false label assignemnt</em>. The ground truth of each of these clips are assumed same as ground truth of the video which may not be the case if the action just happens for a small duration within the entire video.</li>
  <li>The method involved pre-computing optical flow vectors and storing them separately. Also, the training for both the streams was separate implying end-to-end training on-the-go is still a long road.</li>
</ol>

<p><a name="sec-3"></a></p>

<h2 id="summaries">Summaries</h2>

<p>Following papers which are, in a way, evolutions from the two papers (single stream and two stream) which are summarized as below:</p>

<ol>
  <li><a href="#lrcn">LRCN</a></li>
  <li><a href="#c3d">C3D</a></li>
  <li><a href="#attentionandconv3d">Conv3D &amp; Attention</a></li>
  <li><a href="#2streamfusion">TwoStreamFusion</a></li>
  <li><a href="#tsn">TSN</a></li>
  <li><a href="#actionvlad">ActionVlad</a></li>
  <li><a href="#hidden2stream">HiddenTwoStream</a></li>
  <li><a href="#i3d">I3D</a></li>
  <li><a href="#t3d">T3D</a></li>
</ol>

<p>The recurrent theme around these papers can be summarized as follows. All of the papers are improvisations on top of these basic ideas.</p>

<p align="center">
    <img src="/assets/images/actionrec/recurrent_theme_high.png" alt="SegNet Architecture" />
    <br />
    <small>Recurrent theme across papers. <a href="https://arxiv.org/pdf/1705.07750.pdf">Source</a>.</small>
</p>

<p>For each of these papers, I list down their key contributions and explain them.
I also show their benchmark scores on <a href="http://crcv.ucf.edu/data/UCF101.php">UCF101-split1</a>.</p>

<p><a name="lrcn"></a></p>

<h3 id="lrcn">LRCN</h3>

<ul class="no-bullets">
    <li> Long-term Recurrent Convolutional Networks for Visual Recognition and Description </li>
    <li> Donahue et al. </li>
    <li> Submitted on 17 November 2014 </li>
    <li> <a href="https://arxiv.org/abs/1411.4389">Arxiv Link</a></li>
</ul>

<p><em>Key Contributions</em>:</p>

<ul>
  <li>Building on previous work by using RNN as opposed to stream based designs</li>
  <li>Extension of encoder-decoder architecture for video representations</li>
  <li>End-to-end trainable architecture proposed for action recognition</li>
</ul>

<p><em>Explanation</em>:</p>

<p>In a previous work by Ng et al[<a href="#9">9</a>]. authors had explored the idea of using LSTMs on separately trained feature maps to see if it can capture temporal information from clips. Sadly, they conclude that temporal pooling of convoluted features proved more effective than LSTM stacked after trained feature maps. In the current paper, authors build on the same idea of using LSTM blocks (decoder) after convolution blocks(encoder) but using end-to-end training of entire architecture. They also compared RGB and optical flow as input choice and found that a weighted scoring of predictions based on both inputs was the best.</p>

<p align="center">
    <img src="/assets/images/actionrec/LRCNactrec_high.png" alt="2 stream architecture" height="250" float="left" />
      <img src="/assets/images/actionrec/GenericLRCN_high.png" alt="2 stream architecture" hspace="30" height="250" width="50%" float="right" />
    <br />
    <small>Fig 5: Left: LRCN for action recognition. Right: Generic LRCN architecture for all tasks  <a href="https://arxiv.org/pdf/1411.4389.pdf">Source</a>.</small>
</p>

<p><em>Algorithm</em>:</p>

<p>During training, 16 frame clips are sampled from video. The architecture is trained end-to-end with input as RGB or optical flow of 16 frame clips. Final prediction for each clip is the average of predictions across each time step. The final prediction at video level is average of predictions from each clip.</p>

<p><em>Benchmarks (UCF101-split1)</em>:</p>

<table>
  <thead>
    <tr>
      <th>Score</th>
      <th>Comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>82.92</td>
      <td>Weighted score of flow and RGB inputs</td>
    </tr>
    <tr>
      <td>71.1</td>
      <td>Score with just RGB</td>
    </tr>
  </tbody>
</table>

<p><em>My comments</em>:</p>

<p>Even though the authors suggested end-to-end training frameworks, there were still a few drawbacks</p>

<ul>
  <li>False label assignment as video was broken to  clips</li>
  <li>Inability to capture long range temporal information</li>
  <li>Using optical flow meant pre-computing flow features separately</li>
</ul>

<p>Varol et al. in their work[<a href="#10">10</a>] tried to compensate for the stunted temporal range problem by using lower spatial resolution of video and longer clips (60 frames) which led to significantly better performance.</p>

<p><a name="c3d"></a></p>

<h3 id="c3d">C3D</h3>

<ul class="no-bullets">
    <li> Learning Spatiotemporal Features with 3D Convolutional Networks </li>
    <li> Du Tran et al. </li>
    <li> Submitted on 02 December 2014 </li>
    <li> <a href="https://arxiv.org/pdf/1412.0767">Arxiv Link</a></li>
</ul>

<p><em>Key Contributions</em>:</p>

<ul>
  <li>Repurposing 3D convolutional networks as feature extractors</li>
  <li>Extensive search for best 3D convolutional kernel and architecture</li>
  <li>Using deconvolutional layers to interpret model decision</li>
</ul>

<p><em>Explanation</em>:</p>

<p>In this work authors built upon <a href="#singlestream">work</a> by Karpathy et al. However, instead of using 2D convolutions across frames, they used 3D convolutions on video volume. The idea was to train these vast networks on Sports1M and then use them (or an ensemble of nets with different temporal depths) as feature extractors for other datasets. Their finding was a simple linear classifier like SVM on top of ensemble of extracted features worked better than she ttate-of-the-art algorithms. The model performed even better if hand crafted features like iDT were  used  additionally.</p>

<p align="center">
    <img src="/assets/images//actionrec/c3d_high.png" alt="SegNet Architecture" width="100%" float="right" />
    <br />
    <small>Differences in C3D paper and single stream paper <a href="https://arxiv.org/pdf/1412.0767">Source</a>.</small>
</p>

<p>The other interesting part of the work was using deconvolutional layers (explained <a href="https://blog.qure.ai/notes/visualizing_deep_learning">here</a>) to interpret the decisions. Their finding was that the net focussed on spatial appearance in first few frames and tracked the motion in the subsequent frames.</p>

<p><em>Algorithm</em>:</p>

<p>During training, five random 2-second clips are extracted for each video with ground truth as action reported in the entire video. In test time, 10 clips are randomly sampled and predictions across them are averaged for final prediction.</p>

<p align="center">
    <img src="/assets/images//actionrec/trial.gif" alt="SegNet Architecture" height="200" width="200" />
    <br />
    <small>3D convolution where convolution is applied on a spatiotemporal cube.</small>
</p>

<p><em>Benchmarks (UCF101-split1)</em>:</p>

<table>
  <thead>
    <tr>
      <th>Score</th>
      <th>Comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>82.3</td>
      <td>C3D (1 net) + linear SVM</td>
    </tr>
    <tr>
      <td>85.2</td>
      <td>C3D (3 nets) + linear SVM</td>
    </tr>
    <tr>
      <td>90.4</td>
      <td>C3D (3 nets) + iDT + linear SVM</td>
    </tr>
  </tbody>
</table>

<p><em>My comments</em>:</p>

<p>The long range temporal modeling was still a problem. Moreover, training such huge networks is computationally a problem - especially for medical imaging where pre-training from natural images doesn‚Äôt help a lot.</p>

<p><strong>Note</strong>: Around the same time Sun et al.[<a href="#11">11</a>] introduced the concept of factorized 3D conv networks (F<sub>ST</sub>CN), where the authors explored the idea of breaking 3D convolutions into spatial 2D convolutions followed by temporal 1D convolutions. The 1D convolution, placed after 2D conv layer, was implemented as 2D convolution over temporal and channel dimension. The factorized 3D convolutions (F<sub>ST</sub>CN) had comparable results on UCF101 split.</p>

<p align="center">
    <img src="/assets/images/actionrec/fstcn_high.png" alt="SegNet Architecture" width="100%" float="right" />
    <br />
    <small>F<sub>ST</sub>CN paper and the factorization of 3D convolution <a href="https://arxiv.org/pdf/1510.00562.pdf">Source</a>.</small>
</p>

<p><a name="attentionandconv3d"></a></p>

<h3 id="conv3d--attention">Conv3D &amp; Attention</h3>

<ul class="no-bullets">
    <li> Describing Videos by Exploiting Temporal Structure </li>
    <li> Yao et al. </li>
    <li> Submitted on 25 April 2015 </li>
    <li> <a href="https://arxiv.org/abs/1502.08029">Arxiv Link</a></li>
</ul>

<p><em>Key Contributions</em>:</p>

<ul>
  <li>Novel 3D CNN-RNN encoder-decoder architecture which captures local spatiotemporal information</li>
  <li>Use of an attention mechanism within a CNN-RNN encoder-decoder framework to capture global context</li>
</ul>

<p><em>Explanation</em>:</p>

<p>Although this work is not directly related to action recognition, but it was a landmark work in terms of video representations. In this paper the authors use a 3D CNN + LSTM as base architecture for video description task. On top of the base, authors use a pre-trained 3D CNN for improved results.</p>

<p><em>Algorithm</em>:</p>

<p>The set up is almost same as encoder-decoder architecture described in <a href="#lrcn">LRCN</a> with two differences</p>

<ol>
  <li>Instead of passing features from 3D CNN as is to LSTM, 3D CNN feature maps for the clip are concatenated with stacked 2D feature maps for the same set of frames to enrich representation {v<sub>1</sub>, v<sub>2</sub>, ‚Ä¶, v<sub>n</sub>} for each frame i. <em>Note</em>: The 2D &amp; 3D CNN used is a pre-trained one and not trained end-to-end like <a href="#lrcn">LRCN</a></li>
  <li>Instead of averaging temporal vectors across all frames, a weighted average is used to combine the temporal features. The <em>attention  weights</em> are decided based on LSTM output at every time step.</li>
</ol>

<p align="center">
    <img src="/assets/images/actionrec/Larochelle_paper_high.png" alt="Attention Mechanism" widht="60%" />
    <br />
    <small>Attention mechanism for action recognition. <a href="https://arxiv.org/abs/1502.08029">Source</a>.</small>
</p>

<p><em>Benchmarks</em>:</p>

<table>
  <thead>
    <tr>
      <th>Score</th>
      <th>Comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>‚Äì</td>
      <td>Network used for video description prediction</td>
    </tr>
  </tbody>
</table>

<p><em>My comments</em>:</p>

<p>This was one of the landmark work in 2015 introducing attention mechanism for the first time for video representations.</p>

<p><a name="2streamfusion"></a></p>

<h3 id="twostreamfusion">TwoStreamFusion</h3>

<ul class="no-bullets">
    <li> Convolutional Two-Stream Network Fusion for Video Action Recognition </li>
    <li> Feichtenhofer et al. </li>
    <li> Submitted on 22 April 2016 </li>
    <li> <a href="https://arxiv.org/abs/1604.06573">Arxiv Link</a></li>
</ul>

<p><em>Key Contributions</em>:</p>

<ul>
  <li>Long range temporal modeling through better long range losses</li>
  <li>Novel multi-level fused architecture</li>
</ul>

<p><em>Explanation</em>:</p>

<p>In this work, authors use the base two stream architecture with two novel approaches and demonstrate performance increment without any significant increase in size of parameters. The authors explore the efficacy of two major ideas.</p>

<ol>
  <li>
    <p>Fusion of spatial and temporal streams (how and when) - For a task discriminating between brushing hair and brushing teeth - spatial net can capture the spatial dependency in a video (if it‚Äôs hair or teeth) while temporal net can capture presence of periodic motion for each spatial location in video. Hence it‚Äôs important to map spatial feature maps pertaining to say a particular facial region to temporal feature map for the corresponding region. To achieve the same, the nets need to be fused at an early level such that responses at the same pixel position are put in correspondence rather than fusing at end (like in base two stream architecture).</p>
  </li>
  <li>
    <p>Combining temporal net output across time frames so that long term dependency is also modeled.</p>
  </li>
</ol>

<p><em>Algorithm</em>:</p>

<p>Everything from two stream architecture remains almost similar except</p>

<ol>
  <li>As described in the figure below, outputs of conv_5 layer from both streams are fused by conv+pooling. There is yet another fusion at the end layer. The final fused output was used for spatiotemporal loss evaluation.
    <p align="center">
     <img src="/assets/images/actionrec/fusion_strategies_high.png" alt="SegNet Architecture" />
     <br />
     <small>Possible strategies for fusing spatial and temporal streams. The one on right performed better. <a href="https://arxiv.org/abs/1604.06573">Source</a>.</small>
 </p>
  </li>
  <li>For temporal fusion, output from temporal net, stacked across time, fused by conv+pooling was used for temporal loss</li>
</ol>

<p align="center">
    <img src="/assets/images/actionrec/2streamfusion.png" width="100%" alt="SegNet Architecture" />
    <br />
    <small>Two stream fusion architecture. There are two paths one for step 1 and other for step 2 <a href="https://arxiv.org/abs/1604.06573">Source</a>.</small>
</p>

<p><em>Benchmarks (UCF101-split1)</em>:</p>

<table>
  <thead>
    <tr>
      <th>Score</th>
      <th>Comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>92.5</td>
      <td>TwoStreamfusion</td>
    </tr>
    <tr>
      <td>94.2</td>
      <td>TwoStreamfusion + iDT</td>
    </tr>
  </tbody>
</table>

<p><em>My comments</em>:
The authors established the supremacy of the TwoStreamFusion method as it improved the performance over C3D without the extra parameters used in C3D.</p>

<p><a name="tsn"></a></p>

<h3 id="tsn">TSN</h3>

<ul class="no-bullets">
    <li> Temporal Segment Networks: Towards Good Practices for Deep Action Recognition </li>
    <li> Wang et al. </li>
    <li> Submitted on 02 August 2016 </li>
    <li> <a href="https://arxiv.org/abs/1608.00859">Arxiv Link</a></li>
</ul>

<p><em>Key Contributions</em>:</p>

<ul>
  <li>Effective solution aimed at long range temporal modeling</li>
  <li>Establishing the usage of batch normalization, dropout and pre-training as good practices</li>
</ul>

<p><em>Explanation</em>:</p>

<p>In this work authors improved on two streams architecture to produce state-of-the-art results. There were two major differences from the original paper</p>

<ol>
  <li>They suggest sampling clips sparsely across the video to better model long range temporal signal instead of the random sampling across entire video.</li>
  <li>
    <p>For final prediction at video-level authors explored multiple strategies. The best strategy was</p>

    <ol>
      <li>Combining scores of temporal and spatial streams (and other streams if other input modalities are involved) separately by averaging across snippets</li>
      <li>Fusing score of final spatial and temporal scores using weighted average and applying softmax over all classes.</li>
    </ol>
  </li>
</ol>

<p>The other important part of the work was establishing the problem of overfitting (due to small dataset sizes) and demonstrating usage of now-prevalent techniques like batch normalization, dropout and pre-trainign to counter the same. The authors also evaluated two new input modalities as alternate to optical flow - namely warped optical flow and RGB difference.</p>

<p><em>Algorithm</em>:</p>

<p>During training and prediction a video is divided into K segments
of equal durations. Thereafter, snippets are sampled  randomly from each of the K segments. Rest of the steps remained similar to two stream architecture with changes as mentioned above.</p>

<p align="center">
    <img src="/assets/images/actionrec/tsn_high.png" alt="SegNet Architecture" />
    <br />
    <small>Temporal Segment Network architecture. <a href="https://arxiv.org/pdf/1608.00859.pdf">Source</a>.</small>
</p>

<p><em>Benchmarks (UCF101-split1)</em>:</p>

<table>
  <thead>
    <tr>
      <th>Score</th>
      <th>Comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>94.0</td>
      <td>TSN (input RGB + Flow )</td>
    </tr>
    <tr>
      <td>94.2</td>
      <td>TSN (input RGB + Flow + Warped flow)</td>
    </tr>
  </tbody>
</table>

<p><em>My comments</em>:</p>

<p>The work attempted to tackle two big challenges in action recognition - overfitting due to small sizes and long range modeling and the results were really strong. However,the problem of pre-computing optical flow and related input modalities was still a problem at large.</p>

<p><a name="actionvlad"></a></p>

<h3 id="actionvlad">ActionVLAD</h3>

<ul class="no-bullets">
    <li> ActionVLAD: Learning spatio-temporal aggregation for action classification </li>
    <li> Girdhar et al. </li>
    <li> Submitted on 10 April 2017 </li>
    <li> <a href="https://arxiv.org/pdf/1704.02895.pdf">Arxiv Link</a></li>
</ul>

<p><em>Key Contributions</em>:</p>

<ul>
  <li>Learnable video-level aggregation of features</li>
  <li>End-to-end trainable model with video-level aggregated features to capture long term dependency</li>
</ul>

<p><em>Explanation</em>:</p>

<p>In this work, the most notable contribution by the authors is the usage of learnable feature aggregation (VLAD) as compared to normal aggregation using maxpool or avgpool. The aggregation technique is akin to bag of visual words. There are multiple learned anchor-point (say c<sub>1</sub>, ‚Ä¶c<sub>k</sub>) based vocabulary representing k typical action (or sub-action) related spatiotemporal features. The output from each stream in two stream architecture is encoded in terms of k-space ‚Äúaction words‚Äù features - each feature being difference of the output from the corresponding anchor-point for any given spatial or temporal location.</p>

<p align="center">
    <img src="/assets/images/actionrec/actionvlad.png" alt="SegNet Architecture" />
    <br />
    <small>ActionVLAD - Bag of action based visual "words". <a href="https://arxiv.org/pdf/1704.02895.pdf">Source</a>.</small>
</p>

<p>Average or max-pooling represent the entire distribution of points as only a single descriptor which can be sub-optimal for representing an entire video composed of multiple sub-actions. In contrast, the proposed video aggregation represents an entire distribution of descriptors with multiple sub-actions by splitting the descriptor space into k cells and pooling inside each of the cells.</p>

<p align="center">
    <img src="/assets/images/actionrec/pooling_difference_high.png" alt="SegNet Architecture" />
    <br />
    <small>While max or average pooling are good for similar features, they do not not adequately capture the complete distribution of features. ActionVlAD clusters the appearance and motion features and aggregates their residuals from nearest cluster centers. <a href="https://arxiv.org/pdf/1704.02895.pdf">Source</a>.</small>
</p>

<p><em>Algorithm</em>:</p>

<p>Everything from two stream architecture remains almost similar except the usage of ActionVLAD layer. The authors experiment multiple layers to place ActionVLAD layer with the late fusion after conv layers working out as the best strategy.</p>

<p><em>Benchmarks (UCF101-split1)</em>:</p>

<table>
  <thead>
    <tr>
      <th>Score</th>
      <th>Comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>92.7</td>
      <td>ActionVLAD</td>
    </tr>
    <tr>
      <td>93.6</td>
      <td>ActionVLAD + iDT</td>
    </tr>
  </tbody>
</table>

<p><em>My comments</em>:
The use of VLAD as an effective way of pooling was already proved long back. The extension of the same in an end-to-end trainable framework made this technique extremely robust and state-of-the-art for most action recognition tasks in early 2017.</p>

<p><a name="hidden2stream"></a></p>

<h3 id="hiddentwostream">HiddenTwoStream</h3>

<ul class="no-bullets">
    <li> Hidden Two-Stream Convolutional Networks for Action Recognition </li>
    <li> Zhu et al. </li>
    <li> Submitted on 2 April 2017 </li>
    <li> <a href="https://arxiv.org/abs/1704.00389">Arxiv Link</a></li>
</ul>

<p><em>Key Contributions</em>:</p>

<ul>
  <li>Novel architecture for generating optical flow input on-the-fly using a separate network</li>
</ul>

<p><em>Explanation</em>:</p>

<p>The usage of optical flow in the two stream architecture made it mandatory to pre-compute optical flow for each sampled frame before hand thereby affecting storage and speed adversely. This paper advocates the usage of an unsupervised architecture to generate optical flow for a stack of frames.</p>

<p>Optical flow can be regarded as an image reconstruction problem. Given a pair of adjacent frames I<sub>1</sub> and I<sub>2</sub> as input, our CNN generates a flow field V. Then using the predicted flow field V and I<sub>2</sub>, I<sub>1</sub>  can be reconstructed as  I<sub>1</sub><sup>‚Äô</sup> using inverse warping such that difference between I<sub>1</sub> and it‚Äôs reconstruction is minimized.</p>

<p><em>Algorithm</em>:</p>

<p>The authors explored multiple strategies and architectures to generate optical flow with largest fps and least parameters without hurting accuracy much. The final architecture was same as two stream architecture with changes as mentioned:</p>

<ol>
  <li>
    <p>The temporal stream now had the optical flow generation net (MotionNet) stacked on the top of the general temporal stream architectures. The input to the temporal stream was now consequent frames instead of preprocessed optical flow.</p>
  </li>
  <li>
    <p>There‚Äôs an additional multi-level loss for the unsupervised training of MotionNet</p>
  </li>
</ol>

<p>The authors also demonstrate improvement in performance using TSN based fusion instead of conventional architecture for two stream approach.</p>

<p align="center">
    <img src="/assets/images/actionrec/hidden2stream_high.png" alt="SegNet Architecture" />
    <br />
    <small>HiddenTwoStream - MotionNet generates optical flow on-the-fly. <a href="https://arxiv.org/pdf/1704.00389.pdf">Source</a>.</small>
</p>

<p><em>Benchmarks (UCF101-split1)</em>:</p>

<table>
  <thead>
    <tr>
      <th>Score</th>
      <th>Comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>89.8</td>
      <td>Hidden Two Stream</td>
    </tr>
    <tr>
      <td>92.5</td>
      <td>Hidden Two Stream + TSN</td>
    </tr>
  </tbody>
</table>

<p><em>My comments</em>:
The major contribution of the paper was to improve speed and associated cost of prediction. With automated generation of flow, the authors relieved the dependency on slower traditional methods to generate optical flow.</p>

<p><a name="i3d"></a></p>

<h3 id="i3d">I3D</h3>

<ul class="no-bullets">
    <li> Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset </li>
    <li> Carreira et al. </li>
    <li> Submitted on 22 May 2017 </li>
    <li> <a href="https://arxiv.org/abs/1705.07750">Arxiv Link</a></li>
</ul>

<p><em>Key Contributions</em>:</p>

<ul>
  <li>Combining 3D based models into two stream architecture leveraging pre-training</li>
  <li>Kinetics dataset for future benchmarking and improved diversity of action datasets</li>
</ul>

<p><em>Explanation</em>:</p>

<p>This paper takes off from where C3D left. Instead of a single 3D network, authors use two different 3D networks for both the streams in the two stream architecture. Also, to take advantage of pre-trained 2D models the authors repeat the 2D pre-trained weights in the 3rd dimension. The spatial stream input now consists of frames stacked in time dimension instead of single frames as in basic two stream architectures.</p>

<p><em>Algorithm</em>:</p>

<p>Same as basic two stream architecture but with 3D nets for each stream</p>

<!-- <p align="center">
    <img src="/assets/images/segmentation-review/segnet_architecture.png" alt="SegNet Architecture">
    <br>
    <small>Segnet Architecture. <a href="https://arxiv.org/abs/1511.00561">Source</a>.</small>
</p> -->

<p><em>Benchmarks (UCF101-split1)</em>:</p>

<table>
  <thead>
    <tr>
      <th>Score</th>
      <th>Comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>93.4</td>
      <td>Two Stream I3D</td>
    </tr>
    <tr>
      <td>98.0</td>
      <td>Imagenet + Kinetics pre-training</td>
    </tr>
  </tbody>
</table>

<p><em>My comments</em>:</p>

<p>The major contribution of the paper was the demonstration of evidence towards benefit of using pre-trained 2D conv nets. The Kinetics dataset, that was open-sourced along the paper, was the other crucial contribution from this paper.</p>

<p><a name="t3d"></a></p>

<h3 id="t3d">T3D</h3>

<ul class="no-bullets">
    <li> Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification </li>
    <li> Diba et al. </li>
    <li> Submitted on 22 Nov 2017 </li>
    <li> <a href="https://arxiv.org/abs/1711.08200">Arxiv Link</a></li>
</ul>

<p><em>Key Contributions</em>:</p>

<ul>
  <li>Architecture to combine temporal information across variable depth</li>
  <li>Novel training architecture &amp; technique to supervise transfer learning between 2D pre-trained net to 3D net</li>
</ul>

<p><em>Explanation</em>:</p>

<p>The authors extend the work done on I3D but suggest using a single stream 3D DenseNet based architecture with multi-depth temporal pooling layer (Temporal Transition Layer) stacked after dense blocks to capture different temporal depths The multi depth pooling is achieved by pooling with kernels of varying temporal sizes.</p>

<p align="center">
    <img src="/assets/images/actionrec/ttl_layer_high.png" alt="SegNet Architecture" />
    <br />
    <small>TTL Layer along with rest of DenseNet architecture. <a href="https://arxiv.org/abs/1711.08200">Source</a>.</small>
</p>

<p>Apart from the above, the authors also devise a new technique of supervising transfer learning betwenn pre-trained 2D conv nets and T3D. The 2D pre-trianed net and T3D are both presented frames and clips from videos where the clips and videos could be from same video or not. The architecture is trianed to predict 0/1 based on the same and the error from the prediction is back-propagated through the T3D net so as to effectively transfer knowledge.</p>

<p align="center">
    <img src="/assets/images/actionrec/transfer_learning_high.png" alt="SegNet Architecture" />
    <br />
    <small>Transfer learning supervision. <a href="https://arxiv.org/abs/1711.08200">Source</a>.</small>
</p>

<p><em>Algorithm</em>:</p>

<p>The architecture is basically 3D modification to DenseNet [<a href="#12">12</a>] with added variable temporal pooling.</p>

<p><em>Benchmarks (UCF101-split1)</em>:</p>

<table>
  <thead>
    <tr>
      <th>Score</th>
      <th>Comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>90.3</td>
      <td>T3D</td>
    </tr>
    <tr>
      <td>91.7</td>
      <td>T3D + Transfer</td>
    </tr>
    <tr>
      <td>93.2</td>
      <td>T3D + TSN</td>
    </tr>
  </tbody>
</table>

<p><em>My comments</em>:</p>

<p>Although the results don‚Äôt improve on I3D results but that can mostly attributed to much lower model footprint as compared to I3D. The most novel contribution of the paper was the supervised transfer learning technique.</p>

<h2 id="references">References</h2>

<ol>
  <li><a name="1"></a><a href="https://arxiv.org/abs/1708.05038">ConvNet Architecture Search for Spatiotemporal Feature Learning</a> by Du Tran et al.</li>
  <li><a name="2"></a><a href="https://deepmind.com/research/open-source/open-source-datasets/kinetics/">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</a></li>
  <li><a name="3"></a><a href="https://hal.inria.fr/inria-00583818/document">Action recognition by dense trajectories</a> by Wang et. al.</li>
  <li><a name="4"></a><a href="http://www.irisa.fr/vista/Papers/2005_ijcv_laptev.pdf">On space-time interest points</a> by Laptev</li>
  <li><a name="5"></a><a href="http://webee.technion.ac.il/control/info/Projects/Students/2012/Itay%20Hubara%20and%20Amit%20Nishri/Book/Papers-STIP/DollarVSPETS05cuboids.pdf">Behavior recognition via sparse spatio-temporal features</a> by Dollar et al</li>
  <li><a name="6"></a><a href="https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_Action_Recognition_with_2013_ICCV_paper.pdf">Action Recognition with Improved Trajectories</a> by Wang et al.</li>
  <li><a name="7"></a><a href="https://pdfs.semanticscholar.org/52df/a20f6fdfcda8c11034e3d819f4bd47e6207d.pdf">3D Convolutional Neural Networks for Human Action Recognition</a> by Ji et al.</li>
  <li><a name="8"></a><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf">Large-scale Video Classification with Convolutional Neural Networks</a> by Karpathy et al.</li>
  <li><a name="9"></a><a href="https://arxiv.org/abs/1503.08909">Beyond Short Snippets: Deep Networks for Video Classification</a> by Ng et al.</li>
  <li><a name="10"></a><a href="https://arxiv.org/abs/1604.04494">Long-term Temporal Convolutions for Action Recognition</a> by Varol et al.</li>
  <li><a name="11"></a><a href="https://arxiv.org/abs/1510.00562">Human Action Recognition using Factorized Spatio-Temporal Convolutional Networks</a> by Sun et al.</li>
  <li><a name="12"></a><a href="https://arxiv.org/abs/1608.06993">Densely Connected Convolutional Networks</a> by Huang et al.</li>
</ol>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.7.2/Chart.bundle.min.js"></script>

<script type="text/javascript" src="/assets/js/ImageStack.js"></script>

<script type="text/javascript">
    var imageList = getImageList('/assets/images/head_ct_study/stacks/QURE-3', 30);
    var stack = new ImageStack({
    images: imageList,
    height: '15rem',
    width: '15rem'
    });
    $('#volume').append(stack);

    var imageList = getImageList('/assets/images/head_ct_study/stacks/denis_walk_avi', 28);
    var stack = new ImageStack({
    images: imageList,
    height: '15rem',
    width: '20rem'
    });
    $('#video').append(stack);
</script>

<style type="text/css">
    /*Scroll Stuff*/
    .custom-scroll{
      float: none;
      margin: 0 auto;
    }

    .custom-scroll::-webkit-scrollbar-track
    {
      -webkit-box-shadow: inset 0 0 6px rgba(0,0,0,0.3);
      border-radius: 5px;
      background-color: #F5F5F5;
    }

    .custom-scroll::-webkit-scrollbar
    {
      width: 12px;
      background-color: #F5F5F5;
    }

    .custom-scroll::-webkit-scrollbar-thumb
    {
      border-radius: 5px;
      -webkit-box-shadow: inset 0 0 6px rgba(0,0,0,.3);
      background-color: #464646;
    }

    td{
        word-wrap: break-word;
        hyphens: auto;
    }
</style>

:ET